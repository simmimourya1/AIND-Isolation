This script evaluates the performance of the custom heuristic function by
comparing the strength of an agent using iterative deepening (ID) search with
alpha-beta pruning against the strength rating of agents using other heuristic
functions.  The `ID_Improved` agent provides a baseline by measuring the
performance of a basic agent using Iterative Deepening and the "improved"
heuristic (from lecture) on your hardware.  The `Student` agent then measures
the performance of Iterative Deepening and the custom heuristic against the
same opponents.


*************************
 Evaluating: ID_Improved 
*************************

Playing Matches:
----------
  Match 1: ID_Improved vs   Random      Result: 34 to 6
  Match 2: ID_Improved vs   MM_Null     Result: 32 to 8
  Match 3: ID_Improved vs   MM_Open     Result: 25 to 15
  Match 4: ID_Improved vs MM_Improved   Result: 22 to 18
  Match 5: ID_Improved vs   AB_Null     Result: 29 to 11
  Match 6: ID_Improved vs   AB_Open     Result: 26 to 14
  Match 7: ID_Improved vs AB_Improved   Result: 26 to 14


Results:
----------
ID_Improved         69.29%

*************************
  Evaluating: Student0   
*************************

Playing Matches:
----------
  Match 1:  Student0   vs   Random      Result: 37 to 3
  Match 2:  Student0   vs   MM_Null     Result: 30 to 10
  Match 3:  Student0   vs   MM_Open     Result: 25 to 15
  Match 4:  Student0   vs MM_Improved   Result: 23 to 17
  Match 5:  Student0   vs   AB_Null     Result: 27 to 13
  Match 6:  Student0   vs   AB_Open     Result: 28 to 12
  Match 7:  Student0   vs AB_Improved   Result: 20 to 20


Results:
----------
Student0            67.86%

*************************
  Evaluating: Student1   
*************************

Playing Matches:
----------
  Match 1:  Student1   vs   Random      Result: 34 to 6
  Match 2:  Student1   vs   MM_Null     Result: 35 to 5
  Match 3:  Student1   vs   MM_Open     Result: 27 to 13
  Match 4:  Student1   vs MM_Improved   Result: 20 to 20
  Match 5:  Student1   vs   AB_Null     Result: 30 to 10
  Match 6:  Student1   vs   AB_Open     Result: 24 to 16
  Match 7:  Student1   vs AB_Improved   Result: 26 to 14


Results:
----------
Student1            70.00%

*************************
  Evaluating: Student2   
*************************

Playing Matches:
----------
  Match 1:  Student2   vs   Random      Result: 37 to 3
  Match 2:  Student2   vs   MM_Null     Result: 29 to 11
  Match 3:  Student2   vs   MM_Open     Result: 23 to 17
  Match 4:  Student2   vs MM_Improved   Result: 20 to 20
  Match 5:  Student2   vs   AB_Null     Result: 33 to 7
  Match 6:  Student2   vs   AB_Open     Result: 26 to 14
  Match 7:  Student2   vs AB_Improved   Result: 27 to 13


Results:
----------
Student2            69.64%

*************************
  Evaluating: Student3   
*************************

Playing Matches:
----------
  Match 1:  Student3   vs   Random      Result: 33 to 7
  Match 2:  Student3   vs   MM_Null     Result: 32 to 8
  Match 3:  Student3   vs   MM_Open     Result: 25 to 15
  Match 4:  Student3   vs MM_Improved   Result: 23 to 17
  Match 5:  Student3   vs   AB_Null     Result: 28 to 12
  Match 6:  Student3   vs   AB_Open     Result: 24 to 16
  Match 7:  Student3   vs AB_Improved   Result: 23 to 17


Results:
----------
Student3            67.14%

*************************
  Evaluating: Student4   
*************************

Playing Matches:
----------
  Match 1:  Student4   vs   Random      Result: 32 to 8
  Match 2:  Student4   vs   MM_Null     Result: 31 to 9
  Match 3:  Student4   vs   MM_Open     Result: 27 to 13
  Match 4:  Student4   vs MM_Improved   Result: 25 to 15
  Match 5:  Student4   vs   AB_Null     Result: 29 to 11
  Match 6:  Student4   vs   AB_Open     Result: 24 to 16
  Match 7:  Student4   vs AB_Improved   Result: 29 to 11


Results:
----------
Student4            70.36%

This script evaluates the performance of the custom heuristic function by
comparing the strength of an agent using iterative deepening (ID) search with
alpha-beta pruning against the strength rating of agents using other heuristic
functions.  The `ID_Improved` agent provides a baseline by measuring the
performance of a basic agent using Iterative Deepening and the "improved"
heuristic (from lecture) on your hardware.  The `Student` agent then measures
the performance of Iterative Deepening and the custom heuristic against the
same opponents.


*************************
 Evaluating: ID_Improved 
*************************

Playing Matches:
----------
  Match 1: ID_Improved vs   Random      Result: 33 to 7
  Match 2: ID_Improved vs   MM_Null     Result: 27 to 13
  Match 3: ID_Improved vs   MM_Open     Result: 27 to 13
  Match 4: ID_Improved vs MM_Improved   Result: 26 to 14
  Match 5: ID_Improved vs   AB_Null     Result: 26 to 14
  Match 6: ID_Improved vs   AB_Open     Result: 27 to 13
  Match 7: ID_Improved vs AB_Improved   Result: 23 to 17


Results:
----------
ID_Improved         67.50%





*************************
  Evaluating: Student0   
*************************

Playing Matches:
----------
  Match 1:  Student0   vs   Random      Result: 34 to 6
  Match 2:  Student0   vs   MM_Null     Result: 33 to 7
  Match 3:  Student0   vs   MM_Open     Result: 26 to 14
  Match 4:  Student0   vs MM_Improved   Result: 25 to 15
  Match 5:  Student0   vs   AB_Null     Result: 28 to 12
  Match 6:  Student0   vs   AB_Open     Result: 23 to 17
  Match 7:  Student0   vs AB_Improved   Result: 26 to 14


Results:
----------
Student0            69.64%

*************************
  Evaluating: Student1   
*************************

Playing Matches:
----------
  Match 1:  Student1   vs   Random      Result: 34 to 6
  Match 2:  Student1   vs   MM_Null     Result: 30 to 10
  Match 3:  Student1   vs   MM_Open     Result: 25 to 15
  Match 4:  Student1   vs MM_Improved   Result: 24 to 16
  Match 5:  Student1   vs   AB_Null     Result: 26 to 14
  Match 6:  Student1   vs   AB_Open     Result: 25 to 15
  Match 7:  Student1   vs AB_Improved   Result: 27 to 13


Results:
----------
Student1            68.21%

*************************
  Evaluating: Student2   
*************************

Playing Matches:
----------
  Match 1:  Student2   vs   Random      Result: 33 to 7
  Match 2:  Student2   vs   MM_Null     Result: 29 to 11
  Match 3:  Student2   vs   MM_Open     Result: 20 to 20
  Match 4:  Student2   vs MM_Improved   Result: 27 to 13
  Match 5:  Student2   vs   AB_Null     Result: 35 to 5
  Match 6:  Student2   vs   AB_Open     Result: 21 to 19
  Match 7:  Student2   vs AB_Improved   Result: 24 to 16


Results:
----------
Student2            67.50%

*************************
  Evaluating: Student3   
*************************

Playing Matches:
----------
  Match 1:  Student3   vs   Random      Result: 30 to 10
  Match 2:  Student3   vs   MM_Null     Result: 30 to 10
  Match 3:  Student3   vs   MM_Open     Result: 24 to 16
  Match 4:  Student3   vs MM_Improved   Result: 27 to 13
  Match 5:  Student3   vs   AB_Null     Result: 29 to 11
  Match 6:  Student3   vs   AB_Open     Result: 25 to 15
  Match 7:  Student3   vs AB_Improved   Result: 23 to 17


Results:
----------
Student3            67.14%

*************************
  Evaluating: Student4   
*************************

Playing Matches:
----------
  Match 1:  Student4   vs   Random      Result: 32 to 8
  Match 2:  Student4   vs   MM_Null     Result: 32 to 8
  Match 3:  Student4   vs   MM_Open     Result: 28 to 12
  Match 4:  Student4   vs MM_Improved   Result: 25 to 15
  Match 5:  Student4   vs   AB_Null     Result: 29 to 11
  Match 6:  Student4   vs   AB_Open     Result: 21 to 19
  Match 7:  Student4   vs AB_Improved   Result: 22 to 18


Results:
----------
Student4            67.50%



This script evaluates the performance of the custom heuristic function by
comparing the strength of an agent using iterative deepening (ID) search with
alpha-beta pruning against the strength rating of agents using other heuristic
functions.  The `ID_Improved` agent provides a baseline by measuring the
performance of a basic agent using Iterative Deepening and the "improved"
heuristic (from lecture) on your hardware.  The `Student` agent then measures
the performance of Iterative Deepening and the custom heuristic against the
same opponents.


*************************
 Evaluating: ID_Improved 
*************************

Playing Matches:
----------
  Match 1: ID_Improved vs   Random      Result: 35 to 5
  Match 2: ID_Improved vs   MM_Null     Result: 29 to 11
  Match 3: ID_Improved vs   MM_Open     Result: 25 to 15
  Match 4: ID_Improved vs MM_Improved   Result: 26 to 14
  Match 5: ID_Improved vs   AB_Null     Result: 33 to 7
  Match 6: ID_Improved vs   AB_Open     Result: 26 to 14
  Match 7: ID_Improved vs AB_Improved   Result: 26 to 14


Results:
----------
ID_Improved         71.43%

*************************
  Evaluating: Student0   
*************************

Playing Matches:
----------
  Match 1:  Student0   vs   Random      Result: 34 to 6
  Match 2:  Student0   vs   MM_Null     Result: 36 to 4
  Match 3:  Student0   vs   MM_Open     Result: 28 to 12
  Match 4:  Student0   vs MM_Improved   Result: 23 to 17
  Match 5:  Student0   vs   AB_Null     Result: 32 to 8
  Match 6:  Student0   vs   AB_Open     Result: 23 to 17
  Match 7:  Student0   vs AB_Improved   Result: 24 to 16


Results:
----------
Student0            71.43%

*************************
  Evaluating: Student1   
*************************

Playing Matches:
----------
  Match 1:  Student1   vs   Random      Result: 35 to 5
  Match 2:  Student1   vs   MM_Null     Result: 31 to 9
  Match 3:  Student1   vs   MM_Open     Result: 24 to 16
  Match 4:  Student1   vs MM_Improved   Result: 25 to 15
  Match 5:  Student1   vs   AB_Null     Result: 27 to 13
  Match 6:  Student1   vs   AB_Open     Result: 25 to 15
  Match 7:  Student1   vs AB_Improved   Result: 23 to 17


Results:
----------
Student1            67.86%

*************************
  Evaluating: Student2   
*************************

Playing Matches:
----------
  Match 1:  Student2   vs   Random      Result: 31 to 9
  Match 2:  Student2   vs   MM_Null     Result: 35 to 5
  Match 3:  Student2   vs   MM_Open     Result: 27 to 13
  Match 4:  Student2   vs MM_Improved   Result: 23 to 17
  Match 5:  Student2   vs   AB_Null     Result: 30 to 10
  Match 6:  Student2   vs   AB_Open     Result: 25 to 15
  Match 7:  Student2   vs AB_Improved   Result: 24 to 16


Results:
----------
Student2            69.64%

*************************
  Evaluating: Student3   
*************************

Playing Matches:
----------
  Match 1:  Student3   vs   Random      Result: 31 to 9
  Match 2:  Student3   vs   MM_Null     Result: 28 to 12
  Match 3:  Student3   vs   MM_Open     Result: 25 to 15
  Match 4:  Student3   vs MM_Improved   Result: 24 to 16
  Match 5:  Student3   vs   AB_Null     Result: 27 to 13
  Match 6:  Student3   vs   AB_Open     Result: 23 to 17
  Match 7:  Student3   vs AB_Improved   Result: 24 to 16


Results:
----------
Student3            65.00%

*************************
  Evaluating: Student4   
*************************

Playing Matches:
----------
  Match 1:  Student4   vs   Random      Result: 36 to 4
  Match 2:  Student4   vs   MM_Null     Result: 36 to 4
  Match 3:  Student4   vs   MM_Open     Result: 30 to 10
  Match 4:  Student4   vs MM_Improved   Result: 30 to 10
  Match 5:  Student4   vs   AB_Null     Result: 37 to 3
  Match 6:  Student4   vs   AB_Open     Result: 22 to 18
  Match 7:  Student4   vs AB_Improved   Result: 23 to 17


Results:
----------
Student4            76.43%